{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "ner_dictionary = {\"0\": \"person\",    \n",
    "                    \"1\": \"location\",\n",
    "                    \"2\": \"property\",\n",
    "                    \"3\": \"facility\",\n",
    "                    \"4\": \"organization\",\n",
    "                    \"5\": \"Misc\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(filename = 'data/ner_data.csv'):\n",
    "    phrase = []\n",
    "    ner = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            ner.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(ner, dtype=int)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_ner.csv')\n",
    "X_test, Y_test = read_csv('data/test_ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_ner(label):\n",
    "    return ner_dictionary[str(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 6)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \"\"\"\n",
    "    # Split sentence into list of lower case words (â‰ˆ 1 line)\n",
    "    words = (sentence.lower()).split()\n",
    "#     print(words)\n",
    "\n",
    "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "    avg = np.zeros((50,))\n",
    "    \n",
    "    # average the word vectors. You can loop over the words in the list \"words\".\n",
    "    for w in words:\n",
    "        try:\n",
    "            avg += word_to_vec_map[w]\n",
    "#             print(word_to_vec_map[w])\n",
    "        except:\n",
    "            continue\n",
    "#             print(w)\n",
    "    avg = avg / len(words)\n",
    "        \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg =  [ 0.41767667  0.59758667 -0.65344    -0.19065633 -0.14341267 -0.17668333\n",
      " -0.59169667 -0.08378967 -0.286369   -0.09159     0.14554167 -0.48012\n",
      " -0.61809667 -0.35851333  0.52082667 -0.147288   -0.17276533 -0.15958333\n",
      " -0.84262333 -0.07995     0.83005333  0.38693    -0.04865     0.16038267\n",
      " -0.54974333 -1.53546667 -0.269039    0.01381667 -0.22590333  0.11782133\n",
      "  3.3204      0.30733    -0.41791467 -0.39879667  0.06613    -0.25322833\n",
      "  0.19150967  0.50869667  0.240988    0.09266333  0.12594633 -0.01752573\n",
      "  0.545627   -0.44854333 -0.17870133  0.19103633 -0.30942    -0.31141667\n",
      "  0.28803333 -0.25586667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"In Paris for\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "    \n",
    "def print_predictions(X, pred):\n",
    "    print()\n",
    "    for i in range(X.shape[0]):\n",
    "        print(X[i], label_to_ner(int(pred[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding_sequence(idx, X):\n",
    "    if idx == 0:\n",
    "        return str(X[idx]) + str(X[idx + 1]) + str(X[idx + 2])\n",
    "    elif idx == int(X.shape[0]) - 1:\n",
    "        return str(X[idx - 2]) + str(X[idx - 1]) + str(X[idx])\n",
    "    else:\n",
    "        return str(X[idx-1]) +\" \"+ str(X[idx]) +\" \"+ str(X[idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, numpy array of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    \n",
    "    for i in range(0, m):                       # Loop over training examples\n",
    "        sentence = padding_sequence(i, X)\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        words = sentence.lower().split()        \n",
    "        avg = np.zeros((50,))\n",
    "        for w in words:\n",
    "            try:\n",
    "                avg += word_to_vec_map[w]\n",
    "            except:\n",
    "#                 print(w)\n",
    "                continue\n",
    "                \n",
    "        avg = avg/len(words)\n",
    "\n",
    "        # Forward propagation\n",
    "        Z = np.dot(W, avg) + b\n",
    "        A = softmax(Z)\n",
    "        pred[i] = np.argmax(A)\n",
    "#         print(pred[i])\n",
    "        \n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.03, num_iterations = 100):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 5, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Define number of training examples\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 6                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    v_w, v_b, eps = 0, 0, 1e-8\n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        for i in range(0,m):                                # Loop over the training examples\n",
    "            \n",
    "            # Padding words to the i'th training example            \n",
    "            sentence = padding_sequence(i, X)\n",
    "            sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "            \n",
    "            avg = sentence_to_avg(sentence, word_to_vec_map)\n",
    "            # Forward propagate through the softmax layer\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = -np.sum(np.multiply(Y_oh[i], np.log(a)))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            \n",
    "            v_w = v_w + dW**2\n",
    "            v_b = v_b + db**2\n",
    "            \n",
    "            # Update parameters with Adam Gradient Descent\n",
    "            W = W - (learning_rate/np.sqrt(v_w + eps)) * dW\n",
    "            b = b - (learning_rate/np.sqrt(v_b + eps)) * db\n",
    "        \n",
    "        if t % 50 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 2.07270237827\n",
      "Accuracy: 0.948538713195\n",
      "Epoch: 50 --- cost = 3.07315772547\n",
      "Accuracy: 0.948560523446\n",
      "[[ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " ..., \n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.948516902944\n",
      "Test set:\n",
      "Accuracy: 0.947702749739\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.818181818182\n",
      "\n",
      "Queen Misc\n",
      "Anne person\n",
      "Hill person\n",
      "is Misc\n",
      "just Misc\n",
      "few Misc\n",
      "minutes Misc\n",
      "from Misc\n",
      "the Misc\n",
      "Seattle Misc\n",
      "center Misc\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"Queen\", \"Anne\", \"Hill\", \"is\", \"just\", \"few\", \"minutes\", \"from\", \"the\", \"Seattle\", \"center\"])\n",
    "Y_my_labels = np.array([[5],[0],[5],[5],[5],[5],[5],[5],[5],[1],[5]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
